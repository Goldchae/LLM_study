{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰화\n",
    "split() \n",
    "\n",
    "enumerate() : 함수는 반복 가능한(iterable) 객체를 인덱스와 함께 순회할 수 있게 도와주는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나는', '햄스터를', '사랑해.', '왜냐하면', '햄스터는', '귀엽거든.'] {'나는': 0, '햄스터를': 1, '사랑해.': 2, '왜냐하면': 3, '햄스터는': 4, '귀엽거든.': 5} {0: '나는', 1: '햄스터를', 2: '사랑해.', 3: '왜냐하면', 4: '햄스터는', 5: '귀엽거든.'} [0, 1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"나는 햄스터를 사랑해. 왜냐하면 햄스터는 귀엽거든.\"\n",
    "input_split = input_text.split() # split() 텍스트 분리\n",
    "\n",
    "string_index = {word:idx for idx, word in enumerate(input_split)}\n",
    "index_string = {idx:word for idx, word in enumerate(input_split)} \n",
    "\n",
    "input_ids = [string_index[word] for word in string_index]\n",
    "\n",
    "print(input_split, string_index, index_string, input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰ID -> 벡터\n",
    "파이토치가 제공하는 nn.Embedding 클래스\n",
    "\n",
    "\n",
    "torch.tensor(input_ids): input_ids 리스트나 배열을 PyTorch 텐서로 변환(텐서는 PyTorch에서 다양한 수치 연산을 수행하는 기본 데이터 구조)\n",
    "\n",
    "embed_layer: 이는 임베딩 층(embedding layer)을 나타내며, 주로 자연어 처리에서 단어나 토큰을 고정된 크기의 벡터로 매핑하는 데 사용\n",
    "\n",
    "input_ids 길이가 n이고 임베딩 차원이 d라면, input_embeddings의 차원은 [n, d]\n",
    "\n",
    ".unsqueeze(0): 이 메소드는 텐서에 새로운 차원을 추가(0은 차원을 추가할 위치를 의미하며, 이 경우 가장 앞에 차원을 추가하여 텐서의 모양을 [1, n, d]로 변경)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      4\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m \u001b[38;5;66;03m# 벡터 차원\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "embedding_dim = 16 # 벡터 차원\n",
    "# 토큰 임베딩 층 생성\n",
    "embed_layer = nn.Embedding(len(string_index), embedding_dim)# 토큰 ID를 코튼 임베딩으로 변환\n",
    "\n",
    "input_embeddings = embed_layer(torch.tensor(input_ids))\n",
    "input_embeddings = input_embeddings.unsqueeze(0)\n",
    "# 문장 수, 토큰 수, 차원 수\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 절대적 위치 인코딩\n",
    "\n",
    "nn.Embedding: PyTorch의 Embedding 클래스를 사용하여 임베딩 레이어를 생성(이 레이어는 각 정수 인덱스를 고정된 크기의 밀집 벡터로 매핑)\n",
    "\n",
    "max_position: 임베딩을 생성할 최대 위치 값 즉, 이 값은 모델이 처리할 수 있는 최대 시퀀스 길이\n",
    "\n",
    "embedding_dim: 각 위치에 대해 생성될 벡터의 차원\n",
    "\n",
    "torch.arange: 주어진 범위 내의 연속된 정수를 생성 여기서는 입력 시퀀스의 길이(len(input_ids))만큼의 정수를 생성\n",
    "\n",
    "dtype=torch.long: 생성된 정수의 데이터 타입을 long으로 지정\n",
    "\n",
    "unsqueeze(0): 생성된 1차원 배열에 새로운 차원을 추가하여 2차원으로\n",
    "\n",
    "position_embed_layer(position_ids): 앞서 생성된 position_embed_layer를 사용하여, position_ids에 해당하는 위치 벡터를 조회  결과적으로, 각 위치 ID에 대응하는 임베딩 벡터가 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_position = 12\n",
    "\n",
    "# 위치 인코딩 층 생성\n",
    "position_embed_layer = nn.Embedding(max_position, embedding_dim)\n",
    "\n",
    "position_ids = torch.arange(len(input_ids), dtype=torch.long).unsqueeze(0)\n",
    "position_encodings = position_embed_layer(position_ids)\n",
    "\n",
    "# 토큰 임베딩과 위치 인코딩을 더해 최종 입력 임베딩 생성\n",
    "final_embeddings = input_embeddings + position_encodings\n",
    "final_embeddings.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
